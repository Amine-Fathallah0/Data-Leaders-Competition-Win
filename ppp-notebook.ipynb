{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91864,"databundleVersionId":10898562,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Introduction**\n\nThis project tackles the challenge of predicting voting behavior across African nations using demographic data. Through advanced machine learning techniques, particularly CatBoost with optimized feature engineering, we achieved a competitive accuracy of 0.88397 on the public leaderboard. The solution leverages a diverse set of demographic indicators including education level, job type, and location characteristics to create a robust prediction model.\n\nThe key components of our solution include:\n\nCustom feature engineering with weighted encodings\nOptimized CatBoost classifier\nStrategic handling of categorical variables\nCross-validated model evaluation\nEnsemble techniques for improved accuracy\nOur approach demonstrates strong predictive power while maintaining interpretability, making it valuable for both academic research and practical applications in demographic analysis.","metadata":{}},{"cell_type":"markdown","source":"***Table of Contents***\n\n**Project Overview**\n\n\n* Problem Statement\n* Evaluation Metric\n* Best Score Achievement\n\n**Data Analysis**\n\n\n* Dataset Overview\n* Feature Distribution\n* Missing Values Analysis\n*  Feature Correlations\n\n\n**Feature Engineering**\n\n\n* Binary Feature Encoding\n* One-Hot Encoding\n* Ordinal Encoding for Education\n* Target Encoding for Job Types\n* Feature Scaling\n\n**Model Development**\n\n\n* Base Models Comparison\n* Grid Search Optimization\n* Cross-Validation Results\n* Feature Importance Analysis\n\n**Results & Performance**\n\n\n* Model Accuracy: 0.88397\n* Cross-Validation Scores\n* Feature Impact Analysis\n* Performance Visualization\n\n**Implementation Details**\n\n\n* CatBoost Configuration\n* Data Preprocessing Pipeline\n* Model Training Process\n* Prediction Generation\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Project Overview\n- **Challenge**: Predict voting behavior using demographic data\n- **Best Score**: 0.88397\n- **Model**: CatBoost with optimized feature engineering\n- **Key Features**: Education, job type, location characteristics\n\n","metadata":{}},{"cell_type":"markdown","source":"**Problem Statement**\n\nPredict individual voting participation using demographic and socio-economic data\nBinary classification task to determine whether a respondent voted or did not vote\nDataset of 18,000+ entries with multiple predictive features\n\n\n**Evaluation Metric**\n\nAccuracy: Proportion of correct predictions across all instances\nMeasures the model's ability to correctly classify voting participation\nStraightforward metric for binary classification problems\n\n\n**Best Score Achievement**\n\nGoal: Develop a machine learning model with highest possible accuracy\nKey strategies:\n\nRobust feature engineering\nAdvanced model selection and tuning\nHandling potential class imbalance\nComprehensive data preprocessing\n\n\nPotential top-performing models: Gradient Boosting, Random Forest, Logistic Regression","metadata":{}},{"cell_type":"code","source":"# General libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")  # Suppress warnings for a cleaner output\n\n# Preprocessing\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder\nfrom sklearn.impute import KNNImputer\n\n# Feature selection\nfrom sklearn.feature_selection import mutual_info_regression\n\n# Regression models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, VotingRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\n# Classification models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Evaluation metrics\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\n\n# Anomaly detection\nfrom sklearn.ensemble import IsolationForest\n\n# Encoding\nimport category_encoders as ce\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T14:59:28.299363Z","iopub.execute_input":"2025-01-25T14:59:28.299774Z","iopub.status.idle":"2025-01-25T14:59:28.308224Z","shell.execute_reply.started":"2025-01-25T14:59:28.299739Z","shell.execute_reply":"2025-01-25T14:59:28.306718Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Preprocessing\n\n","metadata":{}},{"cell_type":"code","source":"# Encode categories\ncat_cols = ['location_type', 'cellphone_access', 'gender_of_respondent','relationship_with_head', 'marital_status', 'education_level', 'job_type']\nnum_cols = ['household_size', 'age_of_respondent']\nLoad the dataset\ndf = pd.read_csv('/kaggle/input/democracy-in-data-predict-voting-behavior/train1.csv')\n#label encoding for sum features\nbinary_features = [\"vote\", \"location_type\", \"cellphone_access\", \"gender_of_respondent\"]\nle = LabelEncoder()\nfor col in binary_features:\n    df[col] = le.fit_transform(df[col])\n# One-hot encode categorical features\none_hot_features = [\"relationship_with_head\", \"marital_status\", \"job_type\"]\ndf = pd.get_dummies(df, columns=one_hot_features, drop_first=True)\n\n# Define the full category order for education level\neducation_order = [\n    \"No formal education\",\n    \"Primary education\",\n    \"Secondary education\",\n    \"Vocational/Specialised training\",\n    \"Tertiary education\",\n    \"Other/Dont know/RTA\"\n]\n# Apply Ordinal Encoding\nord_enc = OrdinalEncoder(categories=[education_order])\ndf[\"education_level\"] = ord_enc.fit_transform(df[[\"education_level\"]])\n#vote is the target \nX = df.drop(columns=['vote'])\ny = df['vote']\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n# Feature Scaling\nscaler = StandardScaler()\nX_train = pd.DataFrame(scaler.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\nX_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T14:59:28.309575Z","iopub.execute_input":"2025-01-25T14:59:28.309955Z","iopub.status.idle":"2025-01-25T14:59:28.338774Z","shell.execute_reply.started":"2025-01-25T14:59:28.309909Z","shell.execute_reply":"2025-01-25T14:59:28.336801Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **3.Cross Validation**","metadata":{}},{"cell_type":"code","source":"# Define Models\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n    \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"XGBoost\": XGBClassifier(eval_metric='logloss', use_label_encoder=False),\n    \"CatBoost\": CatBoostClassifier(\n        iterations=500,\n        depth=6,  # Reduce depth to prevent overfitting\n        learning_rate=0.05,\n        l2_leaf_reg=10,  # Stronger regularization\n        subsample=0.8\n    ),\n    \"MLP Classifier\": MLPClassifier(\n        hidden_layer_sizes=(100, 50),\n        activation='relu',\n        solver='adam',\n        max_iter=500,\n        random_state=1,\n        early_stopping=True,\n        validation_fraction=0.1,\n        n_iter_no_change=10\n    )\n}\n\n# Train and cross-validate each model\ncv_results = {}\n\nprint(\"\\nModel Training and Cross-Validation:\")\nfor name, model in models.items():\n    scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=5)\n    cv_results[name] = {\n        'mean_accuracy': scores.mean(),\n        'std_accuracy': scores.std()\n    }\n    model.fit(X_train, y_train)\n    print(f\"{name} trained. Mean CV Accuracy: {scores.mean():.2f}, Std Dev: {scores.std():.2f}\")\n\n# Display sorted cross-validation results\nprint(\"\\nCross-Validation Results (Sorted by Mean Accuracy):\")\nsorted_cv_results = sorted(cv_results.items(), key=lambda x: x[1]['mean_accuracy'], reverse=True)\nfor name, result in sorted_cv_results:\n    print(f\"{name} - Mean Accuracy: {result['mean_accuracy']:.2f}, Std Dev: {result['std_accuracy']:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T14:59:28.339477Z","iopub.status.idle":"2025-01-25T14:59:28.339838Z","shell.execute_reply":"2025-01-25T14:59:28.339702Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4.Fine Tuning (Hyper Parameter Optimization)\n","metadata":{}},{"cell_type":"code","source":"# Define parameter grids\nmlp_param_grid = {\n    'hidden_layer_sizes': [(100, 50)],\n    'activation': ['relu'],\n    'solver': ['adam'],\n    'learning_rate': ['constant', 'adaptive'],\n    'max_iter': [500]\n}\n\ngb_param_grid = {\n    'n_estimators': [100, 200],\n    'learning_rate': [0.05, 0.1, 0.2],\n    'max_depth': [3, 5, 7]\n}\n\ncatboost_param_grid = {\n    'iterations': [100, 200],\n    'learning_rate': [0.05, 0.1, 0.2],\n    'depth': [4, 6, 8]\n}                    \n# Perform grid search for each model\nprint(\"Performing Grid Search for MLP Classifier...\")\nmlp_grid = GridSearchCV(\n    MLPClassifier(early_stopping=True, validation_fraction=0.1, n_iter_no_change=10, random_state=1), \n    param_grid=mlp_param_grid, \n    scoring='accuracy', \n    cv=3, \n    n_jobs=-1\n)\nmlp_grid.fit(X_train, y_train)\nprint(f\"Best parameters for MLP Classifier: {mlp_grid.best_params_}\")\n\nprint(\"\\nPerforming Grid Search for Gradient Boosting...\")\ngb_grid = GridSearchCV(\n    GradientBoostingClassifier(random_state=1), \n    param_grid=gb_param_grid, \n    scoring='accuracy', \n    cv=3, \n    n_jobs=-1\n)\ngb_grid.fit(X_train, y_train)\nprint(f\"Best parameters for Gradient Boosting: {gb_grid.best_params_}\")\n\nprint(\"\\nPerforming Grid Search for CatBoost...\")\ncatboost_grid = GridSearchCV(\n    CatBoostClassifier(verbose=0, random_state=1), \n    param_grid=catboost_param_grid, \n    scoring='accuracy', \n    cv=3, \n    n_jobs=-1\n)\ncatboost_grid.fit(X_train, y_train)\nprint(f\"Best parameters for CatBoost: {catboost_grid.best_params_}\")\n\n# Evaluate the best models on the test set\nprint(\"\\nTest Set Evaluation:\")\nbest_models = {\n    'MLP Classifier': mlp_grid.best_estimator_,\n    'Gradient Boosting': gb_grid.best_estimator_,\n    'CatBoost': catboost_grid.best_estimator_\n}\n\nfor name, model in best_models.items():\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"{name} - Accuracy: {accuracy:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T14:59:28.340813Z","iopub.status.idle":"2025-01-25T14:59:28.341224Z","shell.execute_reply":"2025-01-25T14:59:28.341037Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5.Defining and Training the Ensemble\n","metadata":{}},{"cell_type":"code","source":"# Define the ensemble model using the best four models\nensemble_model = VotingClassifier(\n    estimators=[\n        ('mlp', mlp_grid.best_estimator_),\n        ('catboost', catboost_grid.best_estimator_),\n        ('gradient_boosting', gb_grid.best_estimator_),\n        ('logistic_regression', models['Logistic Regression'])\n    ],\n    voting='soft'  # Use 'soft' voting to consider the predicted probabilities\n)\n\n#Train the ensemble model\nensemble_model.fit(X_train, y_train)\n\n# Evaluate the ensemble model using cross-validation\nensemble_scores = cross_val_score(ensemble_model, X_train, y_train, scoring='accuracy', cv=5)\nprint(f\"Ensemble Model - Mean CV Accuracy: {ensemble_scores.mean():.2f}, Std Dev: {ensemble_scores.std():.2f}\")\n\n# Evaluate the ensemble model on the test set\ny_pred_ensemble = ensemble_model.predict(X_test)\nensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)\nprint(f\"Ensemble Model - Test Set Accuracy: {ensemble_accuracy:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T14:59:28.342017Z","iopub.status.idle":"2025-01-25T14:59:28.342751Z","shell.execute_reply":"2025-01-25T14:59:28.342310Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6.Preparing Testing Data","metadata":{}},{"cell_type":"code","source":"# Prepare test data\ndef prepare_test_data(df, train_columns):\n    df = df.copy()\n\n    # Encode binary features\n    binary_features = [\"location_type\", \"cellphone_access\", \"gender_of_respondent\"]\n    le = LabelEncoder()\n    for col in binary_features:\n        df[col] = le.fit_transform(df[col])\n\n    # One-hot encode features\n    one_hot_features = [\"relationship_with_head\", \"marital_status\", \"job_type\"]\n    df = pd.get_dummies(df, columns=one_hot_features, drop_first=True)\n\n    # Ordinal encode education_level\n    education_order = [\n        \"No formal education\",\n        \"Primary education\",\n        \"Secondary education\",\n        \"Vocational/Specialised training\",\n        \"Tertiary education\",\n        \"Other/Dont know/RTA\"\n    ]\n    ord_enc = OrdinalEncoder(categories=[education_order])\n    df[\"education_level\"] = ord_enc.fit_transform(df[[\"education_level\"]])\n\n    # Align columns with train set\n    for col in train_columns:\n        if col not in df.columns:\n            df[col] = 0\n    df = df[train_columns]\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T14:59:28.343588Z","iopub.status.idle":"2025-01-25T14:59:28.344154Z","shell.execute_reply":"2025-01-25T14:59:28.343812Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7.Submitting\n","metadata":{}},{"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/democracy-in-data-predict-voting-behavior/test.csv')\ntrain_columns = X_train.columns\ntest_data_preprocessed = prepare_test_data(test_data, train_columns)\ntest_data_scaled = scaler.transform(test_data_preprocessed)\ntest_data_scaled = pd.DataFrame(test_data_scaled, columns=train_columns)\n\n#Predict using the ENSEEEMBLE model ;)\npredicted = ensemble_model.predict(test_data_scaled)\n\n#Create Submission File\nsubmission = pd.DataFrame({\n    'ID': test_data['ID'],\n    'vote': ['Yes' if p == 1 else 'No' for p in predicted]\n})\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created: submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T14:59:28.345276Z","iopub.status.idle":"2025-01-25T14:59:28.345671Z","shell.execute_reply":"2025-01-25T14:59:28.345491Z"}},"outputs":[],"execution_count":null}]}